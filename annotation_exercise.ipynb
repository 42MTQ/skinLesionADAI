{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sccs5Qb1tG-2"
      },
      "source": [
        "## Projects in Data Science - Exercises for the Annotation lectures\n",
        "\n",
        "In these exercises you will use some generated and exisiting (collected from human observer) data, to practice working with inter-observer agreement.\n",
        "\n",
        "The existing data we are working with are annotations made for skin lesion images from the ISIC archive (https://www.isic-archive.com/). The annotations describe different characteristics of how the lesion look - for example, whether they are asymmetric or not.\n",
        "\n",
        "\n",
        "You can download (group07.csv) the data here: https://github.com/raumannsr/ENHANCE/tree/main/0_data/student/2017-2018\n",
        "\n",
        "\n",
        "Each row is a different image, and each column is a different feature done by a specific annotator. For example: Asymmetry_7_1 is the feature Asymmetry annotated by annotator 7_1. There are 100 lesions, each of which was annotated by six annotators in total, five different features, and six different annotators. The feature are either binary, or ordinal.   \n",
        "\n",
        "These annotations were made in 2017 by students at TU Eindhoven. You can (optionally) read more about the background and how the annotations can be used in these papers:\n",
        "\n",
        "* https://arxiv.org/pdf/1806.08174\n",
        "* https://www.melba-journal.org/papers/2021:020.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ251P0ptDmv",
        "outputId": "dc918d74-e60d-4b34-eb80-2efd6e9eba5b"
      },
      "outputs": [],
      "source": [
        "# Load skin lesion annotations as a dataframe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlDMsdizvxrA"
      },
      "source": [
        "## How often do annotators agree?\n",
        "\n",
        "Select Blue which is a binary feature, for two different annotators. Calculate how often they agree in percent. This is the same as the accuracy metric but you can do this without any imports.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGUWDGuJvuz-",
        "outputId": "e35ac3ed-c732-4099-f8c6-4212d2257bcc"
      },
      "outputs": [],
      "source": [
        "## Select two annotators and compute observed agreement (\"accuracy\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kBI_YhGw9qR"
      },
      "source": [
        "Make a 6x6 array where you loop through the different annotators, and calculate their agreement. You should see that for all annotator pairs, the percentage of agreement is between 80 and 90.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgernQTExsOO"
      },
      "outputs": [],
      "source": [
        "## Loop through the annotators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J91RKtOjx4ug"
      },
      "source": [
        "## How can we interpret this agreement measure?\n",
        "\n",
        "Look at how often each annotator found that the lesion has value 1 for the Blue feature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTk9XedOzG-m",
        "outputId": "fcbc005d-2d1b-4120-9866-19a0682d035b"
      },
      "outputs": [],
      "source": [
        "## Find how often each annotator thought the lesion had the Blue feature\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUq3n7D301ow"
      },
      "source": [
        "Create a \"random\" feature (like an annotator who did not look at the image at all), where the Blue feature occurs as often as in the real annotations. Then again calculate the agreement. Is the result what you would expect?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRdg04WK2nGj",
        "outputId": "f0f4e7e1-742c-4ad9-af09-bf5650887d02"
      },
      "outputs": [],
      "source": [
        "# Create random annotation with the same prevalence of Blue adnd calculate the observed agreement\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCl06zc33NeK"
      },
      "source": [
        "Select only the lesions where at least one annotator thought it had the Blue feature. Calculate the agreements again (as percentage out of 100. What do you notice?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwT0UPCb4dR7",
        "outputId": "e540ceab-b53d-4c76-8530-4035de1b763d"
      },
      "outputs": [],
      "source": [
        "# Find all lesions with at least one Blue annotation, and calculate agreement again (in %)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNQqr0-6J7I"
      },
      "source": [
        "## Cohen's Kappa\n",
        "\n",
        "As you maybe see above, total % of agreement may not reflect what you want to find out about your annotations, if the different values are not occuring equally often.\n",
        "\n",
        "Instead let's look at the Kappa score, which adjusts for this. You can calculate it yourself it from the observed agreement (on all lesions) above. See https://en.wikipedia.org/wiki/Cohen%27s_kappa or the Viera paper on LearnIT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkNHGbdF7bHK",
        "outputId": "c646629f-496e-4cf1-c3fc-02e0fea0d115"
      },
      "outputs": [],
      "source": [
        "# Calculate kappa score \"by hand\" (example)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkZ643ml96eR"
      },
      "source": [
        "Check your answer against the Kappa score available in sklearn.metrics.\n",
        "\n",
        "Note that this score is only for two annotators and categorical variables. For extensions of the Cohen's Kappa you will need to adapt your own function, or use other packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG43Mxfd-H2m",
        "outputId": "740b9f7a-17c6-4a1b-cece-2ecd498c69af"
      },
      "outputs": [],
      "source": [
        "# Calculate Kappa score with sklearn\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
